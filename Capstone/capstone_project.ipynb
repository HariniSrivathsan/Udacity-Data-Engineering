{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project focuses on building an ETL pipeline using I-94 immigration data and tempertaure data from various cities as the two data-sources. The data is read, cleansed, appropriately merged (joined) and finally stored in disk in partitions (for optimal querying).\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from misc_utils import MiscUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_obj = MiscUtils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Data from I-94 immigration records are aggregated by destination city. Likewise, data from city-temperature records are aggregated by city. Then, the two tables are joined (on destination city) to form a fact table.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The I94 immigration data comes from the US National Tourism and Trade Office. It is provided in SAS7BDAT format which is a binary database storage format. Some relevant attributes include:\n",
    "\n",
    "    i94yr = 4 digit year\n",
    "    i94mon = numeric month\n",
    "    i94cit = 3 digit code of origin city\n",
    "    i94port = 3 character code of destination USA city\n",
    "    arrdate = arrival date in the USA\n",
    "    i94mode = 1 digit travel code\n",
    "    depdate = departure date from the USA\n",
    "    i94visa = reason for immigration\n",
    "\n",
    "The temperature data comes from Kaggle. It is provided in csv format. Some relevant attributes include:\n",
    "\n",
    "    AverageTemperature = average temperature\n",
    "    City = city name\n",
    "    Country = country name\n",
    "    Latitude= latitude\n",
    "    Longitude = longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the I94 data here\n",
    "# for speed/convenience, we use only one month data: Apr 2016.\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "i94_df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3096313, 28)\n"
     ]
    }
   ],
   "source": [
    "print(i94_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the temperature data\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "cityTemp_df = pd.read_csv(fname, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8599212, 7)\n"
     ]
    }
   ],
   "source": [
    "print(cityTemp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4  1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityTemp_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Clean the Data\n",
    "\n",
    "Some of the cleansing steps taken are:\n",
    "- Remove observations that have invalid port-code\n",
    "- Round up the average-temperature and avg-temperature-uncertainty values (large decimal values is not useful).\n",
    "- Remove observations where average temperature is null\n",
    "- drop duplicate locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session to read SAS7BDAT formatted files.\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|       admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null|1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null| 3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS| 6.66643185E8|   93|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and Cleaning SAS Label Descriptions\n",
    "Read the given SAS descriptions and have the port-code and values in a separate .txt file. Then, create a data-dictionary to have a good mapping between the city-code and city-name. While doing so, ensure that the names do not have extra spaces (as in the original SAS label description), strip from both sides etc. This code is in a separate file called misc_utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all valid I94 ports from SAS Label descriptions\n",
    "valid_i94_ports_dict = mu_obj.clean_ports('valid_i94_ports.txt')\n",
    "#valid_i94_ports_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all entries with invalid/unknown port-codes and \n",
    "# only keep those with valid port-codes.\n",
    "def clean_i94_data(i94_file):\n",
    "    raw_i94_df = spark.read.format('com.github.saurfang.sas.spark').load(i94_file)\n",
    "    validated_i94_df = raw_i94_df.filter(raw_i94_df.i94port.isin(list(valid_i94_ports_dict.keys())))\n",
    "    return validated_i94_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only Apr 2016 I-94 dataset (for short run cycle)\n",
    "sample_i94_file = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat' \n",
    "sample_cleansed_i94_df = clean_i94_data(sample_i94_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in cleansed i94 dataset: 3088576\n",
      "Number of columns in cleansed i94 dataset: 28\n"
     ]
    }
   ],
   "source": [
    "mu_obj.spark_df_shape(sample_cleansed_i94_df, 'cleansed i94')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "| 18.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MI|20555.0|  57.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247103803E10|00602|      B1|\n",
      "| 19.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  63.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1953.0|09302016|  null|  null|     AZ|9.247139923E10|00602|      B2|\n",
      "| 20.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  57.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247161383E10|00602|      B2|\n",
      "| 21.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20553.0|  46.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1970.0|09302016|  null|  null|     AZ|9.247079603E10|00602|      B2|\n",
      "| 22.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20562.0|  48.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1968.0|09302016|  null|  null|     AZ|9.247848973E10|00608|      B1|\n",
      "| 23.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20671.0|  52.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1964.0|09302016|  null|  null|     TK|9.250139443E10|00001|      B2|\n",
      "| 24.0|2016.0|   4.0| 101.0| 101.0|    TOR|20545.0|    1.0|     MO|20554.0|  33.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1983.0|09302016|  null|  null|     MQ|9.249090503E10|03348|      B2|\n",
      "| 27.0|2016.0|   4.0| 101.0| 101.0|    BOS|20545.0|    1.0|     MA|20549.0|  58.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1958.0|04062016|     M|  null|     LH|9.247876383E10|00422|      B1|\n",
      "| 28.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20549.0|  56.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1960.0|04062016|     F|  null|     LH|9.247890033E10|00422|      B1|\n",
      "| 29.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20561.0|  62.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1954.0|09302016|     M|  null|     AZ|9.250378143E10|00614|      B2|\n",
      "| 30.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NJ|20578.0|  49.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1967.0|09302016|     M|  null|     OS|9.247020943E10|00089|      B2|\n",
      "| 31.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NY|20611.0|  43.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1973.0|09302016|     M|  null|     OS|9.247128923E10|00089|      B2|\n",
      "| 33.0|2016.0|   4.0| 101.0| 101.0|    HOU|20545.0|    1.0|     TX|20554.0|  53.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1963.0|09302016|     F|  null|     TK|9.250930163E10|00033|      B2|\n",
      "| 34.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  48.0|    2.0|  1.0|20160401|     TIA| null|      G|   null|   null|   null| 1968.0|09302016|     M|  null|     AZ|9.247042023E10|00602|      B2|\n",
      "| 35.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  74.0|    2.0|  1.0|20160401|     TIA| null|      T|   null|   null|   null| 1942.0|09302016|     F|  null|     TK|  6.69712185E8|    1|      B2|\n",
      "| 36.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20561.0|  37.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1979.0|09302016|     M|  null|     TK|9.250625823E10|00001|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_cleansed_i94_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in raw temperature dataset: 8599212\n",
      "Number of columns in raw temperature dataset: 7\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Number of rows in filtered temperature dataset: 8235082\n",
      "Number of columns in filtered temperature dataset: 7\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01|5.7879999999999985|           3.6239999999999997|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|            10.644|           1.2830000000000001|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01|14.050999999999998|                        1.347|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01|            16.082|                        1.396|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Number of rows in cleansed temperature dataset: 3490\n",
      "Number of columns in cleansed temperature dataset: 7\n",
      "+----------+--------------------+-----------------------------+-----------+-------------+--------+---------+\n",
      "|        dt|  AverageTemperature|AverageTemperatureUncertainty|       City|      Country|Latitude|Longitude|\n",
      "+----------+--------------------+-----------------------------+-----------+-------------+--------+---------+\n",
      "|1743-11-01|               3.264|                        1.665|  Allentown|United States|  40.99N|   74.56W|\n",
      "|1779-11-01|0.011999999999999985|                        2.714|     Atyrau|   Kazakhstan|  47.42N|   50.92E|\n",
      "|1825-01-01|  26.069000000000003|                         2.16|    Bintulu|     Malaysia|   2.41N|  113.30E|\n",
      "|1825-01-01|              26.517|           2.5839999999999996|Butterworth|     Malaysia|   5.63N|  100.09E|\n",
      "|1845-01-01|              24.995|                        1.871|     Cainta|  Philippines|  15.27N|  120.83E|\n",
      "+----------+--------------------+-----------------------------+-----------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean temperature data\n",
    "raw_temperature_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "mu_obj.spark_df_shape(raw_temperature_df, 'raw temperature')\n",
    "raw_temperature_df.show(5)\n",
    "\n",
    "# Filter out entries with null average temperature\n",
    "filtered_temperature_df = raw_temperature_df.filter(raw_temperature_df.AverageTemperature != 'null')\n",
    "print(\"\\n\")\n",
    "mu_obj.spark_df_shape(filtered_temperature_df, 'filtered temperature')\n",
    "filtered_temperature_df.show(5)\n",
    "\n",
    "# Remove duplicate locations\n",
    "cleansed_temperature_df = filtered_temperature_df.dropDuplicates(['City', 'Country'])\n",
    "print(\"\\n\")\n",
    "mu_obj.spark_df_shape(cleansed_temperature_df, 'cleansed temperature')\n",
    "cleansed_temperature_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf()\n",
    "def get_i94port(city):\n",
    "    '''\n",
    "    Input: City name\n",
    "    Output: Corresponding i94port\n",
    "    '''\n",
    "    for key, _ in valid_i94_ports_dict.items():\n",
    "        if city.strip().lower() in valid_i94_ports_dict[key].lower():\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----------+-------------+--------+---------+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|       City|      Country|Latitude|Longitude|i94port|\n",
      "+----------+------------------+-----------------------------+-----------+-------------+--------+---------+-------+\n",
      "|1743-11-01|              3.26|                         1.67|  Allentown|United States|  40.99N|   74.56W|   null|\n",
      "|1779-11-01|              0.01|                         2.71|     Atyrau|   Kazakhstan|  47.42N|   50.92E|   null|\n",
      "|1825-01-01|             26.07|                         2.16|    Bintulu|     Malaysia|   2.41N|  113.30E|   null|\n",
      "|1825-01-01|             26.52|                         2.58|Butterworth|     Malaysia|   5.63N|  100.09E|   null|\n",
      "|1845-01-01|              25.0|                         1.87|     Cainta|  Philippines|  15.27N|  120.83E|   null|\n",
      "+----------+------------------+-----------------------------+-----------+-------------+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add i94_port code based on city name\n",
    "# And, round the temperatures to 2 digits after decimal point.\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "temperature_df = cleansed_temperature_df.withColumn(\"i94port\", get_i94port(cleansed_temperature_df.City))\n",
    "temperature_df = temperature_df.withColumn(\"AverageTemperature\", func.round(temperature_df[\"AverageTemperature\"], 2))\n",
    "temperature_df = temperature_df.withColumn(\"AverageTemperatureUncertainty\", func.round(temperature_df[\"AverageTemperatureUncertainty\"], 2))\n",
    "\n",
    "temperature_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+--------+-------------+--------+---------+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|    City|      Country|Latitude|Longitude|i94port|\n",
      "+----------+------------------+-----------------------------+--------+-------------+--------+---------+-------+\n",
      "|1852-07-01|             15.49|                          1.4|   Perth|    Australia|  31.35S|  114.97E|    PER|\n",
      "|1828-01-01|             -1.98|                         2.55| Seattle|United States|  47.42N|  121.97W|    SEA|\n",
      "|1743-11-01|              2.77|                         1.91|Hamilton|       Canada|  42.59N|   80.73W|    HAM|\n",
      "|1849-01-01|               7.4|                          2.7| Ontario|United States|  34.56N|  116.76W|    ONT|\n",
      "|1821-11-01|              2.32|                         2.38| Spokane|United States|  47.42N|  117.24W|    SPO|\n",
      "+----------+------------------+-----------------------------+--------+-------------+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove entries with no i94_port code\n",
    "temperature_df = temperature_df.filter(temperature_df.i94port != 'null')\n",
    "temperature_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a long time. Do not run often.\n",
    "# mu_obj.spark_df_shape(temperature_df, 'temperature_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "\n",
    "The immigration table comes from the I-94 Immigration data. Each report (entry) contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries). From this, the following columns are extracted.\n",
    "\n",
    "    i94yr = 4 digit year\n",
    "    i94mon = numeric month\n",
    "    i94cit = 3 digit code of origin city\n",
    "    i94port = 3 character code of destination city\n",
    "    arrdate = arrival date\n",
    "    i94mode = 1 digit travel code\n",
    "    depdate = departure date\n",
    "    i94visa = reason for immigration\n",
    "\n",
    "The second table is the city temperature data gotten from Kaggle. The following fields are considered:\n",
    "\n",
    "    i94port = 3 character code of destination city (mapped from immigration data during cleanup step)\n",
    "    AverageTemperature = average temperature\n",
    "    City = city name\n",
    "    Country = country name\n",
    "    Latitude= latitude\n",
    "    Longitude = longitude\n",
    "\n",
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on i94port:\n",
    "\n",
    "    i94yr = 4 digit year\n",
    "    i94mon = numeric month\n",
    "    i94cit = 3 digit code of origin city\n",
    "    i94port = 3 character code of destination city\n",
    "    arrdate = arrival date\n",
    "    i94mode = 1 digit travel code\n",
    "    depdate = departure date\n",
    "    i94visa = reason for immigration\n",
    "    AverageTemperature = average temperature of destination city\n",
    "\n",
    "The tables will be saved to Parquet files partitioned by city (i94port).\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Here are the steps that are taken:\n",
    "- A spark dataframe is computed from cleansed I94 data\n",
    "- A spark dataframe is computed from cleansed Temperature data\n",
    "- Create an immigration dimension table and store as Parquet files (partitioned by i94port)\n",
    "- Create a temperature dimension table and store as Parquet files (partitioned by i94port)\n",
    "- Compute a fact table by joining the above two tables based on i94port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to I94 immigration data \n",
    "#immigration_data = '/data/18-83510-I94-Data-2016/*.sas7bdat'\n",
    "immigration_data = '/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "\n",
    "# Clean I94 immigration data and store as Spark dataframe\n",
    "df_immigration = clean_i94_data(immigration_data)\n",
    "\n",
    "# Extract columns for immigration dimension table\n",
    "immigration_table = df_immigration.select([\"i94yr\", \"i94mon\", \"i94cit\", \"i94port\", \"arrdate\", \"i94mode\", \"depdate\", \"i94visa\"])\n",
    "\n",
    "# Write immigration dimension table to parquet files partitioned by i94port\n",
    "immigration_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns for temperature dimension table\n",
    "temperature_dim_tbl = temperature_df.select([\"AverageTemperature\", \"City\", \"Country\", \"Latitude\", \"Longitude\", \"i94port\"])\n",
    "\n",
    "# Write temperature dimension table to parquet files partitioned by i94port\n",
    "temperature_dim_tbl.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views of the immigration and temperature data\n",
    "df_immigration.createOrReplaceTempView(\"immigration_view\")\n",
    "temperature_df.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "# Create the fact table by joining the immigration and temperature views\n",
    "fact_table = spark.sql('''\n",
    "SELECT immigration_view.i94yr as year,\n",
    "       immigration_view.i94mon as month,\n",
    "       immigration_view.i94cit as city,\n",
    "       immigration_view.i94port as i94port,\n",
    "       immigration_view.arrdate as arrival_date,\n",
    "       immigration_view.depdate as departure_date,\n",
    "       immigration_view.i94visa as reason,\n",
    "       temp_view.AverageTemperature as temperature,\n",
    "       temp_view.Latitude as latitude,\n",
    "       temp_view.Longitude as longitude\n",
    "FROM immigration_view\n",
    "JOIN temp_view ON (immigration_view.i94port = temp_view.i94port)\n",
    "''')\n",
    "\n",
    "# Write fact table to parquet files partitioned by i94port\n",
    "fact_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/results/fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "The only check I do here is to count the final size of the dataframes and ensure they are not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for immigration table with 3088576 records\n",
      "Data quality check passed for temperature table with 175 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "def quality_check(df, description):\n",
    "    '''\n",
    "    Input: Spark dataframe, description of Spark datafram\n",
    "    Output: Print outcome of data quality check\n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return 0\n",
    "\n",
    "# Perform data quality check\n",
    "quality_check(df_immigration, \"immigration table\")\n",
    "quality_check(temperature_df, \"temperature table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "\n",
    "The immigration table comes from the I-94 Immigration data. Each report (entry) contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries). From this, the following columns are extracted.\n",
    "\n",
    "    i94yr = 4 digit year\n",
    "    i94mon = numeric month\n",
    "    i94cit = 3 digit code of origin city\n",
    "    i94port = 3 character code of destination city\n",
    "    arrdate = arrival date\n",
    "    i94mode = 1 digit travel code\n",
    "    depdate = departure date\n",
    "    i94visa = reason for immigration\n",
    "\n",
    "The second table is the city temperature data gotten from Kaggle. The following fields are considered:\n",
    "\n",
    "    i94port = 3 character code of destination city (mapped from immigration data during cleanup step)\n",
    "    AverageTemperature = average temperature\n",
    "    City = city name\n",
    "    Country = country name\n",
    "    Latitude= latitude\n",
    "    Longitude = longitude\n",
    "\n",
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on i94port:\n",
    "\n",
    "    i94yr = 4 digit year\n",
    "    i94mon = numeric month\n",
    "    i94cit = 3 digit code of origin city\n",
    "    i94port = 3 character code of destination city\n",
    "    arrdate = arrival date\n",
    "    i94mode = 1 digit travel code\n",
    "    depdate = departure date\n",
    "    i94visa = reason for immigration\n",
    "    AverageTemperature = average temperature of destination city\n",
    "\n",
    "The tables will be saved to Parquet files partitioned by city (i94port).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "The datasets given are very large. The number of observations run into millions. To preprocess, analyse and perform data-intensive computations on them, Spark is an ideal technology to use.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "Both I-94 Immigration dataset and Temperature dataset have monthly aggregates, it would be preferabe to update the datasets on a monthly basis.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " In this case, one should ensure we run increase the number of spark nodes appropriately. 'Appropriately' - A given SLA would determine (trail and error) how many nodes are needed to meet the SLA.\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    " An ETL tool like Airflow can be used the previous night to ensure (with retries, email notification etc) that data is properly staged, pre-processed and necessary KPIs/metrics are extracted and available to dashboard program (Tableau..) in a nice partitioned manner.\n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    " In this case, it can be stored in some scalable, high-performance db like RedShift, Snowflake or BigQuery. It is to be noted that the data must be properly partitioned, indexed and given the right permissions to glitch-free access my large number of consumers. Furthermore, only the necessary fields are to be kept and the rest can be discarded.\n",
    " \n",
    " * Purpose of this project.\n",
    " The final data model contains data from both I-94 immigration arrival information and the respective cities temperature details. This information will be assimilated monthly. This will act as a rich source of information to :\n",
    "     1. Understand the travel patterns during different months of the years.\n",
    "     2. Understand the reasons why certain ports have high traffic than the others.\n",
    "     3. Helps in capacity-planning for city/airport officials and be prepared for the varying inflows.\n",
    "     4. Helps in demographic analysis (based on Visa information of immigrants) to see what percentage of influx (to a city) are students, professionals, family folsk, etc and maybe some amount of attribution could be done for seasonality and important days of a year (Christmas, Thanksgiving, graduation, ...)\n",
    "\n",
    "\n",
    "In order for the downstream analysts to answer such questions, a clean, high-quality dataset must be prepared and ready to be accessed. Hence, the purpose of this project is enable downstream data-analysts/scientists to be able to access quality data (without redundancies and other errors). Perhaps, for faster access to data-analysts and reporting folks, we could export this data into a RedShift Database so that they could visualize using Tableau or something similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
